defaults:
  - ppo_trainer
  - _self_

# 大模型配置
actor_rollout_ref:
  model:
    path: /mnt/bn/ai4sml-exploration/xuyiran/yr/qwen_weight/Qwen3-8B-Base  # 大模型路径
  
  actor:
    strategy: fsdp
    ppo_mini_batch_size: 8
    ppo_epochs: 1
    loss_agg_mode: mean  # 新增：loss聚合方式
    
    optim:
      lr: 1e-6
      lr_scheduler_type: cosine
      weight_decay: 0.0
  
  rollout:
    n: 1  # 离线模式不需要多次采样
    mode: sync
    log_prob_micro_batch_size: 32

# 算法配置
algorithm:
  _target_: verl.trainer.config.AlgoConfig
  
  adv_estimator: grpo  # 使用GRPO
  gamma: 1.0
  lam: 1.0
  norm_adv_by_std_in_grpo: true
  
  # 重要性采样（用于纠正小大模型分布差异）
  rollout_is_threshold: 2.0
  rollout_is_threshold_lower: null  # 自动设置为1/threshold
  rollout_is_level: token
  rollout_is_mode: truncate
  rollout_is: true  # 启用IS权重应用
  
  # KL配置
  use_kl_in_reward: false  # GRPO一般不使用KL
  
  # 其他
  use_pf_ppo: false

# 数据配置（关键！）
data:
  # 使用离线rollout数据
  use_offline_rollout: true
  
  # 离线数据路径
  train_files:
    - ./small_model_rollouts/small_model_rollouts.jsonl
  
  # 验证数据用原始数据
  val_files:
    - data/math/val.parquet
  
  # 每个prompt的samples数（应该与小模型采样一致）
  offline_samples_per_prompt: 50
  
  # Batch配置
  train_batch_size: 8  # 8 prompts × 50 samples = 400 samples per batch
  gen_batch_size: 8    # 与train_batch_size一致
  val_batch_size: 64
  
  # 长度限制
  max_prompt_length: 1024
  max_response_length: 2048
  
  # DataLoader配置
  dataloader_num_workers: 4
  shuffle: true
  
  # Reward key
  reward_fn_key: null

# Trainer配置
trainer:
  project_name: small_to_large_grpo
  experiment_name: qwen_0.5b_to_7b_math
  
  total_epochs: 3
  total_training_steps: null  # 自动计算
  
  nnodes: 1
  n_gpus_per_node: 8
  
  # 保存和验证
  save_freq: 100
  test_freq: 50
  val_before_train: true
  
  # Critic warmup
  critic_warmup: 0
  
  # Balance batch
  balance_batch: true
  
  # Logging
  logger: ["console", "wandb"]
  log_val_generations: 5  # 验证时记录5个样本
  
  # Checkpoint
  default_local_dir: ./checkpoints/large_model_grpo
  default_hdfs_dir: null
  del_local_ckpt_after_load: false
  max_actor_ckpt_to_keep: 3
  
  # Resume
  resume_mode: auto
  resume_from_path: null
  
  # Device
  device: cuda

# Reward配置
reward_model:
  enable: false  # 离线数据已包含reward
  enable_resource_pool: false

# Critic配置（GRPO不需要critic）
critic:
  enable: false

# Reference policy配置（如果不用KL可以不启用）
actor_rollout_ref:
  actor:
    use_kl_loss: false

# Ray配置
ray_kwargs:
  ray_init:
    num_cpus: 32
    
# Profiler配置
global_profiler:
  _target_: verl.utils.profiler.ProfilerConfig
  tool: null
  steps: null

# Transfer queue配置
transfer_queue:
  enable: false